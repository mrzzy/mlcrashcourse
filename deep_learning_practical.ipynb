{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlcrashcourse - Deep Learning Practical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import randint\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (InputLayer, Dense, BatchNormalization, Dense, Dropout, \n",
    "                                     Conv2D, Flatten, MaxPool2D)\n",
    "from tensorflow.keras.metrics import categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the problem\n",
    "\n",
    "Link to dataset: https://www.kaggle.com/zalando-research/fashionmnist\n",
    "\n",
    "In this practical we will be using the **Fashion MNIST** dataset. The task is to classify the image into the different fashion classes/labels. \n",
    "\n",
    "> Basically, given an image of a boot, the model should be able to tell me that that is a boot.\n",
    "\n",
    "| No. Label | Text Label |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "This is a supervised problem because there is a expected output we want to obtain from the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training examples, 10000 test examples\n"
     ]
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "# load the data using keras datasets\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "print(f\"{len(train_images)} training examples, {len(test_images)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at a random image and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a: T-shirt/top\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb2701aacc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = randint(0, len(train_images))\n",
    "image = train_images[idx]\n",
    "label_i = train_labels[idx]\n",
    "print(\"This is a:\", label_map[label_i])\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "Neural Networks are fussy with the data that they take in, so we need to do some preprocessing:\n",
    "1. Apply Feature Scaling to the images\n",
    "2. One hot encode the labels\n",
    "\n",
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzy/.conda/envs/ml/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/zzy/.conda/envs/ml/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/zzy/.conda/envs/ml/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform feature scaling with standard scaler \n",
    "# np.reshape() is needed to ensure that the np arrays have the correct shape\n",
    "n_images = len(train_images)\n",
    "scaler = StandardScaler()\n",
    "flat_train_images = np.reshape(train_images, (n_images, -1))\n",
    "\n",
    "# we need to tell the scaler about what data it will be dealing with\n",
    "scaler.fit(flat_train_images)\n",
    "\n",
    "# Define a function to scale features\n",
    "def scale_features(images):\n",
    "    flat_images = np.reshape(images, (len(images), -1))\n",
    "    flat_features = scaler.transform(flat_images)\n",
    "    features = np.reshape(flat_features, (len(images), 28, 28, 1))\n",
    "    return features \n",
    "\n",
    "# Scale both train and test images\n",
    "train_features = scale_features(train_images)\n",
    "test_features = scale_features(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1e9698518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[idx], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb1e967f208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_features[idx].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zzy/.conda/envs/ml/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# we need to tell the encoder about what data it will be dealing with\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.reshape(train_labels, (len(train_labels), 1)))\n",
    "\n",
    "# convert labels into one hot encoding\n",
    "def encode_labels(labels):\n",
    "    labels = np.reshape(labels, (len(labels), 1))\n",
    "    features = encoder.transform(labels)\n",
    "    return features\n",
    "                        \n",
    "train_one_hot_labels = encode_labels(train_labels)\n",
    "test_one_hot_labels = encode_labels(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_hot_labels[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "Now we build the Neural Network model that we will train to classify fashion images.\n",
    "Instead of directly building the model, we create a class that exposes its hyperparameters.\n",
    "This will make hyperparameters tuning later easier.\n",
    "\n",
    "A function that adds a convolution block to your model is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a convolution block to the given sequential model if enabled\n",
    "# layers specify how many convolution layers to use\n",
    "# the scale parameter allows us to scale up the convolution layers\n",
    "def add_conv_block(model, layers, scale, dropout=0.2):\n",
    "        for n_base_filters in range(2, layers * 2 + 1, 2):\n",
    "            model.add(Conv2D(int(n_base_filters * scale), (3, 3),\n",
    "                             padding=\"same\",activation=\"relu\"))\n",
    "            model.add(Conv2D(int(n_base_filters * scale), (3, 3),\n",
    "                             padding=\"same\",activation=\"relu\"))\n",
    "            model.add(Conv2D(int(n_base_filters * scale), (3, 3), \n",
    "                             strides=(2,2), padding=\"same\",activation=\"relu\"))        \n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(dropout))\n",
    "            \n",
    "        model.add(Flatten())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a neural network model\n",
    "class NNModel:\n",
    "    # Create a model with the given hyperparametersz\n",
    "    def __init__(self, \n",
    "                 n_layers,\n",
    "                 learning_rate,\n",
    "                 n_units=64,\n",
    "                 conv_blocks=0,\n",
    "                 conv_scale=0,\n",
    "                 display_summary=False):\n",
    "        \n",
    "        self.input_shape = (28, 28, 1)\n",
    "        self.n_units = n_units\n",
    "        self.n_layers = n_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.conv_scale = conv_scale\n",
    "        self.conv_blocks = conv_blocks\n",
    "        \n",
    "        self.backend_model = self.build()\n",
    "        if display_summary: self.backend_model.summary()\n",
    "        \n",
    "    # Fit the model to the given data\n",
    "    # x - features, y - one hot encoded labels\n",
    "    def fit(self, train_x, train_y, valid_data, n_epochs, batch_size=64):\n",
    "        self.backend_model.fit(train_x, train_y,\n",
    "                               validation_data=valid_data,\n",
    "                               epochs=n_epochs,\n",
    "                               batch_size=batch_size)\n",
    "\n",
    "    # Generate predictions using the given features\n",
    "    def predict(self, input_x):\n",
    "        predict_probs = self.backend_model.predict(input_x, \n",
    "                                                 batch_size=64)\n",
    "        predictions = np.argmax(predict_probs, axis=-1)\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    # Build the model\n",
    "    def build(self):\n",
    "        K.clear_session()\n",
    "        \n",
    "        # Build model architecture\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=self.input_shape))\n",
    "        # Convolution layers\n",
    "        add_conv_block(model, \n",
    "                       layers=self.conv_blocks,\n",
    "                       scale=self.conv_scale)\n",
    "        # Dense layers (The brain part)\n",
    "        for i in range(self.n_layers): \n",
    "            model.add(Dense(self.n_units, activation=\"relu\"))\n",
    "            \n",
    "        # Output layer - classfication 10 classes\n",
    "        model.add(Dense(10, activation=\"sigmoid\"))\n",
    "\n",
    "        # Build model\n",
    "        model.compile(optimizer=Adam(lr=self.learning_rate),\n",
    "                      loss=categorical_crossentropy,\n",
    "                      metrics=[categorical_accuracy])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "Lets build a model with our model class and train it.\n",
    "As usual, there are hyperparameters in or model that we have to tune.\n",
    "We train for 3 epochs, which means that we tell the model to learn from the training data over 3 passes.\n",
    "\n",
    "> In real world, we train for way more epochs (30-200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NNModel(n_layers=2,\n",
    "                n_units=64, \n",
    "                conv_blocks=0,\n",
    "                conv_scale=12,\n",
    "                learning_rate=3e-3, \n",
    "                display_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 8s 135us/sample - loss: 0.4061 - categorical_accuracy: 0.8565 - val_loss: 0.4106 - val_categorical_accuracy: 0.8522\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.3385 - categorical_accuracy: 0.8758 - val_loss: 0.3837 - val_categorical_accuracy: 0.8619\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.3095 - categorical_accuracy: 0.8862 - val_loss: 0.4047 - val_categorical_accuracy: 0.8559\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_features, train_one_hot_labels, \n",
    "                 valid_data=(test_features, test_one_hot_labels),\n",
    "                 n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluting the model\n",
    "One we have have trained the model, we need to evalute how when its doing. We will use the `categorical_accuracy` metric as a yard stick of evaluating how the model is doing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 85.59% accurate\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_features)\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(f\"The model is {accuracy * 100.0}% accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating and Experimenting\n",
    "Now we make changes to the hyperparameters of models and evalute.\n",
    "And then do it again (iterating), trying to get models with higher accuracy.\n",
    "\n",
    "For today, we will only have time for one hyperparamer\n",
    "How about we add some convolution layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 24)        240       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 24)        5208      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 24)        5208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 14, 14, 24)        96        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 24)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4704)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                301120    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 316,682\n",
      "Trainable params: 316,634\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 17s 279us/sample - loss: 0.4260 - categorical_accuracy: 0.8463 - val_loss: 0.3937 - val_categorical_accuracy: 0.8646\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 16s 269us/sample - loss: 0.2727 - categorical_accuracy: 0.8991 - val_loss: 0.2945 - val_categorical_accuracy: 0.8943\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 16s 274us/sample - loss: 0.2266 - categorical_accuracy: 0.9149 - val_loss: 0.2914 - val_categorical_accuracy: 0.9051\n",
      "The model is 90.51% accurate\n"
     ]
    }
   ],
   "source": [
    "model = NNModel(n_layers=2,\n",
    "                n_units=64, \n",
    "                conv_blocks=1,\n",
    "                conv_scale=12,\n",
    "                learning_rate=3e-3,\n",
    "                display_summary=True)\n",
    "                \n",
    "hist = model.fit(train_features, train_one_hot_labels, \n",
    "                 valid_data=(test_features, test_one_hot_labels),\n",
    "                 batch_size=64,\n",
    "                 n_epochs=3)\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "accuracy = accuracy_score(predictions, test_labels)\n",
    "print(f\"The model is {accuracy * 100.0}% accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are on a time limit today thats all that we will will try. But i suggest that you try other values at home.\n",
    "\n",
    ">Neural Networks are able scale to large amount of data, but require large amount of computing resources use effectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
